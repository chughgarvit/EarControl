# -*- coding: utf-8 -*-
"""engauge.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qGDz1B8_4c6J1t_tj1KMWY9d2ASDef9D
"""

import os
import numpy as np
import pandas as pd
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn.manifold
import tensorflow as tf
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight



sns.set_context('poster')
import importlib
import sys

sys.path.append('/content/ezyzip')
print(sys.path)
import raw_data_processing_new
import data_pre_processing
import simclr_models
import simclr_utitlities
import transformations
importlib.reload(transformations)
from transformations import rotation_transform_vectorized

# Function to create a combined action column
def create_combined_action_column(file_path):
    # Read the CSV file into a DataFrame
    df = pd.read_csv(file_path)

    # Create the 'combined_action' column by combining 'posture' and 'action'
    df['combined_action'] = df['Posture'] + "_" + df['micro action']
    df['combined_action_num'] = pd.factorize(df['combined_action'])[0]
    # Display the first few rows to confirm the new column
    print(df[['Posture', 'micro action', 'combined_action']].head())

    # Optionally, save the modified DataFrame back to a new CSV file
    df.to_csv(file_path, index=False)
    print("Combined action column created and saved to 'modified_file.csv'.")

# Set the path to your CSV file
file_path = "ezyzip/test_run/combined_output.csv"

# Call the function
create_combined_action_column(file_path)

# Adjust these paths as needed
working_directory = 'ezyzip/test_run/'
csv_file_name = 'combined_output.csv'
csv_file_path = os.path.join(working_directory, csv_file_name)

if not os.path.exists(csv_file_path):
    raise FileNotFoundError(f"File not found: {csv_file_path}")

# Load data
data = pd.read_csv(csv_file_path)
print("Dataset Loaded Successfully. Here's a preview:")
print(data.head())
unique_counts = data["combined_action_num"].value_counts()
print("uniique values in combined action:",unique_counts)


unique_counts = data["combined_action"].value_counts()
print("uniique values in action:",unique_counts)
# Suppose these are your feature columns
feature_cols = [
    "Gyro_x_x", "Gyro_y_x", "Gyro_z_x",
    "Acc_x_x", "Acc_y_x", "Acc_z_x",
    "Gyro_x_y","Gyro_y_y","Gyro_z_y",
    "Acc_x_y","Acc_y_y","Acc_z_y"
]
X = data[feature_cols].values

# Numeric label column
y = data["combined_action_num"].values

# Combine into a single dataframe
df = pd.DataFrame(X, columns=feature_cols)
df['label'] = y

print("Data shape:", df.shape)
print("Unique labels:", df['label'].unique())

print(df.isnull().any())
df = df.fillna(method="ffill")  # Example: forward fill
print(df.isnull().any())

# Check distribution
label_counts = df['label'].value_counts()
print("Label distribution:\n", label_counts)

# Option A: Class Weights for Training
# -----------------------------------
# We'll compute class weights for the label array to use in the .fit() method

unique_labels = np.unique(y)
class_weights = compute_class_weight(
    class_weight='balanced',  # automatically inversely proportional to class freq
    classes=unique_labels,
    y=y
)
class_weights_dict = {label: weight for label, weight in zip(unique_labels, class_weights)}
print("Computed class weights:", class_weights_dict)

# Option B (Alternate): Oversampling or Undersampling
# For instance, use SMOTE or random oversampling on minority classes.
# We won't show that code here for brevity, but libraries like 'imblearn' can help.

def get_random_split(dataset, test_size=0.2, random_state=42):
    """
    Splits the DataFrame into train and test.
    """
    X = dataset.iloc[:, :-1]
    y = dataset.iloc[:, -1]
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state, stratify=y # use stratify=y for balanced splits
    )
    return (
        pd.concat([X_train, y_train], axis=1),
        pd.concat([X_test, y_test], axis=1)
    )

train_df, test_df = get_random_split(df)
print("Train shape:", train_df.shape)
print("Test shape:", test_df.shape)


# --- Sliding window function (if needed) ---
def sliding_window_np(X, window_size, shift, stride=1, flatten=None):
    X = np.asarray(X)
    num_windows = (len(X) - window_size + shift) // shift
    windows = []
    for i in range(num_windows):
        start_index = i * shift
        this_window = X[start_index : start_index + window_size : stride]
        if flatten is not None:
            this_window = flatten(this_window)
        windows.append(this_window)
    return np.array(windows)

from collections import Counter
def get_mode(array_1d):
    c = Counter(array_1d)
    return max(c, key=c.get)

def prepare_sliding_window_data(df, window_size=200, shift=200, normalize=True):
    """
    Convert df to windowed data.
    df: last column is 'label'.
    """
    X_ = df.iloc[:, :-1].values  # features
    y_ = df.iloc[:, -1].values   # labels

    # Create windows
    X_win = sliding_window_np(X_, window_size, shift)
    y_win = sliding_window_np(y_, window_size, shift, flatten=get_mode)

    # Normalize
    if normalize:
        mean = X_win.mean(axis=(0,1))
        std  = X_win.std(axis=(0,1)) + 1e-6
        X_win = (X_win - mean) / std

    return X_win, y_win

# Adjust window_size and shift to reflect actual activity durations
window_size = 50
shift = 25
X_train_win, y_train_win = prepare_sliding_window_data(train_df, window_size, shift)
X_test_win,  y_test_win  = prepare_sliding_window_data(test_df,  window_size, shift)

# Make a final train/val split
X_train, X_val, y_train, y_val = train_test_split(
    X_train_win, y_train_win,
    test_size=0.2,
    random_state=42,
    stratify=y_train_win  # keep label proportion in val
)

print("X_train:", X_train.shape, "y_train:", y_train.shape)
print("X_val:  ", X_val.shape,   "y_val:  ", y_val.shape)
print("X_test: ", X_test_win.shape, "y_test:", y_test_win.shape)

# 1) Define transformations
transform_funcs = [
    transformations.rotation_transform_vectorized,
    # Add other transformations if desired, e.g. transformations.random_scaling, ...
]
transformation_function = simclr_utitlities.generate_composite_transform_function_simple(transform_funcs)

# 2) Hyperparameters
batch_size = 256
epochs_pretrain = 500
temperature = 0.1
initial_learning_rate = 0.01
decay_steps = 1000
lr_schedule = tf.keras.optimizers.schedules.CosineDecay(
    initial_learning_rate,
    decay_steps
)
optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)

# 3) Create SimCLR Model
input_shape = (X_train.shape[1], X_train.shape[2])  # e.g., (window_size, num_features)
base_model = simclr_models.create_base_model(input_shape, model_name="base_model")
simclr_model = simclr_models.attach_simclr_head(base_model)
simclr_model.summary()

# 4) Train SimCLR
trained_simclr_model, epoch_losses = simclr_utitlities.simclr_train_model(
    simclr_model,
    X_train,  # unlabeled for contrastive
    optimizer,
    batch_size,
    transformation_function,
    temperature=temperature,
    epochs=epochs_pretrain,
    is_trasnform_function_vectorized=True,
    verbose=1
)

# 5) Save if desired
import datetime
start_time_str = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
simclr_model_save_path = os.path.join(working_directory, f"{start_time_str}_simclr.keras")
trained_simclr_model.save(simclr_model_save_path)

# Plot training loss
plt.figure()
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# 1) Build classification head on top of the pretrained SimCLR base
num_classes = 12
linear_model = simclr_models.create_linear_model_from_base_model(
    trained_simclr_model,
    output_shape=num_classes,
    intermediate_layer=7  # depends on your architecture
)

# Freeze the base if your helper function doesn't already do it
# Make sure only the new dense head is trainable
for layer in linear_model.layers[:-1]:
    layer.trainable = False

linear_model.summary()

# 2) One-hot encode labels
y_train_oh = to_categorical(y_train, num_classes)
y_val_oh   = to_categorical(y_val,   num_classes)
y_test_oh  = to_categorical(y_test_win, num_classes)

# 3) Compile with class weights for imbalance handling
linear_model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

checkpoint_path = os.path.join(working_directory, f"{start_time_str}_linear_eval_best.keras")
checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(
    checkpoint_path, monitor='val_loss', save_best_only=True, verbose=1
)

history_linear_eval = linear_model.fit(
    X_train, y_train_oh,
    validation_data=(X_val, y_val_oh),
    epochs=500,
    batch_size=64,
    class_weight=class_weights_dict,  # <--- handle imbalance via computed weights
    callbacks=[checkpoint_cb],
    verbose=1
)

# 4) Evaluate
best_linear_model = tf.keras.models.load_model(checkpoint_path)
test_loss, test_acc = best_linear_model.evaluate(X_test_win, y_test_oh)
print("Linear Eval - Test Accuracy:", test_acc)

# 1) Reload the best linear evaluation model as the starting point
fine_tune_model = tf.keras.models.load_model(checkpoint_path)

# 2) Unfreeze some (or all) layers. Example: unfreeze everything
for layer in fine_tune_model.layers:
    layer.trainable = True

fine_tune_model.summary()

# 3) Compile with a smaller LR and class weights
fine_tune_lr = 1e-3  # smaller LR to avoid forgetting
fine_tune_model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=fine_tune_lr),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# 4) Train further (fine-tune)
checkpoint_path_finetune = os.path.join(working_directory, f"{start_time_str}_finetuned_best.keras")
checkpoint_cb_finetune = tf.keras.callbacks.ModelCheckpoint(
    checkpoint_path_finetune, monitor='val_loss', save_best_only=True, verbose=1
)

history_finetune = fine_tune_model.fit(
    X_train, y_train_oh,
    validation_data=(X_val, y_val_oh),
    epochs=500,  # or more
    batch_size=64,
    class_weight=class_weights_dict,  # keep handling imbalance
    callbacks=[checkpoint_cb_finetune],
    verbose=1
)

# 5) Final Evaluation
best_finetuned_model = tf.keras.models.load_model(checkpoint_path_finetune)
test_loss, test_acc = best_finetuned_model.evaluate(X_test_win, y_test_oh)
print("Fine-Tuned - Test Accuracy:", test_acc)

# Plot training curves for linear evaluation
plt.figure(figsize=(8, 5))
plt.plot(history_linear_eval.history['loss'], label='Train Loss (Linear Eval)')
plt.plot(history_linear_eval.history['val_loss'], label='Val Loss (Linear Eval)')
plt.title('Linear Evaluation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Plot fine-tuning curves
plt.figure(figsize=(8, 5))
plt.plot(history_finetune.history['loss'], label='Train Loss (Fine-tune)')
plt.plot(history_finetune.history['val_loss'], label='Val Loss (Fine-tune)')
plt.title('Fine-tuning Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

from sklearn.manifold import TSNE

# Extract embeddings from an intermediate layer
# You can use your simclr_models.extract_intermediate_model_from_base_model if you want
intermediate_model = simclr_models.extract_intermediate_model_from_base_model(
    best_finetuned_model,
    intermediate_layer=7
)

embeddings = intermediate_model.predict(X_test_win)
perplexity = 10
tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42, verbose=1)
tsne_proj = tsne.fit_transform(embeddings)

# Plot t-SNE
labels_argmax = np.argmax(y_test_oh, axis=1)
num_classes = np.max(labels_argmax) + 1

plt.figure(figsize=(10, 8))
sns.scatterplot(
    x=tsne_proj[:,0],
    y=tsne_proj[:,1],
    hue=labels_argmax,
    palette=sns.color_palette("hsv", num_classes),
    alpha=0.8
)
plt.legend()
plt.title("t-SNE of Fine-Tuned Embeddings")
plt.show()

