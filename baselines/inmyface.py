# -*- coding: utf-8 -*-
"""inmyface.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j3yCNfYIN20jqTkJ6wJhdYizgWrsyLYs
"""

import pandas as pd
import numpy as np
import os
import glob
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from scipy import stats
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Function to check data integrity
def is_data_clean(participant_id):
    return participant_id not in corrupted_participants

# Filter participants
participants = [f'P{i}' for i in range(30) if is_data_clean(f'P{i}')]
print(f"Clean participants: {participants}")

# ------------------------------
# 3. Define the Data Loading Function
# ------------------------------
def load_participant_activity(folder_id, file_id, activity):
    base_path = os.path.join(clean_data_path, folder_id, 'EARBUDS')
    file_activity = activity  # No replacement needed since names match filenames

    # Construct file paths using file_activity
    imu_left_path = os.path.join(base_path, f"{file_id}-{file_activity}-imu-left.csv")
    imu_right_path = os.path.join(base_path, f"{file_id}-{file_activity}-imu-right.csv")

    # Check if all files exist
    for path in [imu_left_path, imu_right_path]:
        if not os.path.exists(path):
            raise FileNotFoundError(f"File not found: {path}")

    # Load CSVs
    imu_left = pd.read_csv(imu_left_path)
    imu_right = pd.read_csv(imu_right_path)

    # Convert 'timestamp' to float and drop NaNs
    for df in [imu_left, imu_right]:
        df['timestamp'] = pd.to_numeric(df['timestamp'], errors='coerce').astype(float)
        df.dropna(subset=['timestamp'], inplace=True)

    # Scale IMU data
    for col in ['ax', 'ay', 'az']:
        imu_left[col] *= 0.061
        imu_right[col] *= 0.061
    for col in ['gx', 'gy', 'gz']:
        imu_left[col] *= 17.5
        imu_right[col] *= 17.5

    # Merge PPG and IMU data based on nearest timestamp
    merged_left = pd.merge_asof(
        imu_left.sort_values('timestamp'),
        on='timestamp',
        direction='nearest'
    )

    merged_right = pd.merge_asof(
        imu_right.sort_values('timestamp'),
        on='timestamp',
        direction='nearest'
    )

    return merged_left, merged_right

# ------------------------------
# 4. Normalize Data
# ------------------------------
scaler = StandardScaler()

def normalize_data(df):
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    df_copy = df.copy()  # To avoid SettingWithCopyWarning
    df_copy[numeric_cols] = scaler.fit_transform(df_copy[numeric_cols])
    return df_copy

# ------------------------------
# 5. Feature Extraction Function
# ------------------------------
def extract_features(df):
    features = {}
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        features[f'{col}_mean'] = df[col].mean()
        features[f'{col}_std'] = df[col].std()
        features[f'{col}_max'] = df[col].max()
        features[f'{col}_min'] = df[col].min()
        features[f'{col}_median'] = df[col].median()
        features[f'{col}_25th'] = df[col].quantile(0.25)
        features[f'{col}_75th'] = df[col].quantile(0.75)
    return features

# ------------------------------
# 6. Define Activity Labels (Corrected)
# ------------------------------
activity_labels = {
    'still': 0,
    'nod': 1,
    'shake': 1,
    'tilt': 1,
    'eyes_lr': 1,          # Corresponds to 'eyes-lr'
    'eyes_ud': 1,          # Corresponds to 'eyes-ud'
    'brow_lowerer': 1,     # Corresponds to 'brow-lowerer'
    'brow_raiser': 1,      # Corresponds to 'brow-raiser'
    'right_eye_wink': 1,   # Corresponds to 'right-eye-wink'
    'left_eye_wink': 1,    # Corresponds to 'left-eye-wink'
    'chewing': 1,
    'speaking': 1,
    'swallowing': 1,       # If present in the dataset
    'walking': 2,          # Corrected from 'walk'
    'running': 2           # Corrected from 'run'
}

# ------------------------------
# 6. Define Activities List (Corrected)
# ------------------------------
activities = [
    'still',
    'nod',
    'shake',
    'tilt',
    'eyes_lr',
    'eyes_ud',
    'brow_lowerer',
    'brow_raiser',
    'right_eye_wink',
    'left_eye_wink',
    'chewing',
    'speaking',
    'swallowing',
    'walking',    # Corrected from 'walk'
    'running'     # Corrected from 'run'
]


activities = [
    'still', 'nod', 'shake', 'tilt', 'vertical_eye', 'horizontal_eye',
    'brow_raise', 'brow_lower', 'right_eye_wink', 'left_eye_wink',
    'lip_puller', 'chin_raiser', 'mouth_stretch', 'chewing',
    'speaking', 'walk', 'run'
]

# ------------------------------
# 7. Load and Process All Data
# ------------------------------
data = []
missing_files = []

for participant in participants:
    file_id = participant[1:]  # Extract numeric part, e.g., 'P0' -> '0'
    for activity in activities:
        try:
            # Load data
            ppg_left, ppg_right = load_participant_activity(participant, file_id, activity)

            # Normalize data
            ppg_left_norm = normalize_data(ppg_left)
            ppg_right_norm = normalize_data(ppg_right)

            # Extract features
            features_left = extract_features(ppg_left_norm)
            features_right = extract_features(ppg_right_norm)

            # Combine features
            combined_features = {**features_left, **features_right}

            # Assign label
            label = activity_labels.get(activity, -1)
            if label == -1:
                print(f"Undefined label for activity '{activity}'. Skipping.")
                continue  # Skip undefined activities

            combined_features['label'] = label
            combined_features['participant'] = participant  # Optional
            combined_features['activity'] = activity        # Optional

            # Append to data list
            data.append(combined_features)
            print(f"Successfully processed {participant} - {activity}")

        except FileNotFoundError as fnf_error:
            print(f"File not found for {participant} - {activity}: {fnf_error}")
            missing_files.append((participant, activity, str(fnf_error)))
        except Exception as e:
            print(f"Error processing {participant} - {activity}: {e}")
            missing_files.append((participant, activity, str(e)))

# Summarize missing files
print("\nSummary of Missing Files:")
for entry in missing_files:
    print(f"Participant: {entry[0]}, Activity: {entry[1]}, Error: {entry[2]}")

# Convert to DataFrame
df = pd.DataFrame(data)
print("\nFinal DataFrame Head:")
print(df.head())
print(f"Total samples collected: {len(df)}")

# ------------------------------
# 8. Handle Missing Data and Outliers
# ------------------------------
# Check for missing values
print("\nMissing values in each column:")
print(df.isnull().sum())

# Option 1: Drop rows with any missing values
df.dropna(inplace=True)

# Option 2: Fill missing values using forward fill (if appropriate)
# df.fillna(method='ffill', inplace=True)

print("\nAfter handling missing values:")
print(df.isnull().sum())

# Handle outliers using Z-score
z_scores = np.abs(stats.zscore(df.drop(['label', 'participant', 'activity'], axis=1)))
df = df[(z_scores < 3).all(axis=1)]
print(f"Data shape after outlier removal: {df.shape}")

# ------------------------------
# 9. Feature Scaling
# ------------------------------
# Separate features and labels
X = df.drop(['label', 'participant', 'activity'], axis=1)
y = df['label']

# Initialize the scaler
scaler = StandardScaler()

# Fit and transform the features
X_scaled = scaler.fit_transform(X)

# Convert back to DataFrame for convenience
X_scaled = pd.DataFrame(X_scaled, columns=X.columns)
print("\nScaled Features Head:")
print(X_scaled.head())

# ------------------------------
# 10. Split the Data into Training and Testing Sets
# ------------------------------
# Split data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

print(f"\nTraining samples: {X_train.shape[0]}")
print(f"Testing samples: {X_test.shape[0]}")

# ------------------------------
# 11. Train a Machine Learning Model (Random Forest Classifier)
# ------------------------------
# Initialize the model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
rf_model.fit(X_train, y_train)

# ------------------------------
# 12. Evaluate the Model
# ------------------------------
# Make predictions on the test set
y_pred = rf_model.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"\nRandom Forest Accuracy: {accuracy:.2f}")

# Classification Report
try:
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, target_names=['Still', 'Facial/Head Motion', 'Full-Body Motion']))
except ValueError as ve:
    print("\nError in Classification Report:")
    print(ve)

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8,6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Still', 'Facial/Head Motion', 'Full-Body Motion'],
            yticklabels=['Still', 'Facial/Head Motion', 'Full-Body Motion'])
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix - Random Forest')
plt.show()

print("\nLabel Distribution:")
print(y.value_counts())

# participant = 'P0'
# file_id = participant[1:]  # '0'
# base_path = os.path.join(clean_data_path, participant, 'EARBUDS')
# available_files = os.listdir(base_path)
# print(f"Available files for {participant}:")
# print(available_files)

